{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AI Chat with Ollama! ü§ñ\n",
    "\n",
    "Welcome to this fun tutorial where we'll learn how to chat with AI models using Python! We'll be using something called **Ollama**, which lets us run AI models right on our own computers.\n",
    "\n",
    "In this tutorial, we'll learn:\n",
    "1. How to have a simple chat with an AI\n",
    "2. How to make the AI give us specific types of answers (like Yes/No or Multiple Choice)\n",
    "3. How to save our conversations with the AI\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup üõ†Ô∏è\n",
    "\n",
    "First, we need to make sure we have all the tools we need. We'll use a few Python packages:\n",
    "- `ollama`: To talk to our AI models\n",
    "- `pydantic`: To help us get structured responses from the AI\n",
    "\n",
    "Let's install these packages if we don't have them already:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple Chat with AI üí¨\n",
    "\n",
    "Let's start by importing the packages we need and creating a simple function to chat with our AI. We'll make it super easy to use!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from ollama import chat\n",
    "\n",
    "def chat_with_ai(prompt: str, model: str = \"llama2\", temperature: float = 0.7):\n",
    "    \"\"\"\n",
    "    Have a simple chat with an AI model!\n",
    "    \n",
    "    Args:\n",
    "        prompt: Your question or message to the AI\n",
    "        model: Which AI model to use (default is llama2)\n",
    "        temperature: How creative the AI should be (0.0 = very focused, 1.0 = very creative)\n",
    "    \n",
    "    Returns:\n",
    "        The AI's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send our message to the AI\n",
    "        response = chat(\n",
    "            model=model,\n",
    "            options={\"temperature\": temperature},\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        # Get the AI's response\n",
    "        return response[\"message\"][\"content\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Oops! Something went wrong: {e}\"\n",
    "\n",
    "# Let's test our function with a simple question!\n",
    "response = chat_with_ai(\"What is artificial intelligence? Explain it to a high school student.\")\n",
    "print(\"AI's response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Our Conversations üìù\n",
    "\n",
    "It's often useful to save our conversations with the AI for later. Let's create a function that helps us do that!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI's response:\n",
      " Oops! Something went wrong: model \"llama2\" not found, try pulling it first (status code: 404)\n",
      "\n",
      "Chat saved to: results/chats/chat_20250622_114409.txt\n"
     ]
    }
   ],
   "source": [
    "def save_chat(model: str, prompt: str, response: str):\n",
    "    \"\"\"\n",
    "    Save our chat to a file so we can look at it later!\n",
    "    \n",
    "    Args:\n",
    "        model: The AI model we used\n",
    "        prompt: What we asked the AI\n",
    "        response: What the AI answered\n",
    "    \"\"\"\n",
    "    # Create a folder to store our chats if it doesn't exist\n",
    "    results_dir = os.path.join(\"results\", \"chats\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a filename with the current time\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = os.path.join(results_dir, f\"chat_{timestamp}.txt\")\n",
    "    \n",
    "    # Save the conversation\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Model: {model}\\n\")\n",
    "        f.write(f\"\\nOur Question:\\n{prompt}\\n\")\n",
    "        f.write(f\"\\nAI's Answer:\\n{response}\\n\")\n",
    "    \n",
    "    print(f\"\\nChat saved to: {filename}\")\n",
    "\n",
    "# Let's try asking another question and save it!\n",
    "prompt = \"What are three fun projects a high school student could do to learn about AI?\"\n",
    "response = chat_with_ai(prompt)\n",
    "print(\"AI's response:\\n\", response)\n",
    "save_chat(\"llama2\", prompt, response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Getting Specific Answers from AI üéØ\n",
    "\n",
    "Sometimes we want the AI to give us very specific types of answers, like:\n",
    "- Just \"Yes\" or \"No\"\n",
    "- True or False\n",
    "- Multiple choice (A, B, C, or D)\n",
    "\n",
    "This is called \"structured output\" because we're telling the AI exactly how to structure its answer. Let's see how to do this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Is Python a programming language?\n",
      "Answer: Oops! Something went wrong: model \"llama2\" not found, try pulling it first (status code: 404)\n",
      "\n",
      "Statement: The Earth is flat.\n",
      "Answer: Oops! Something went wrong: model \"llama2\" not found, try pulling it first (status code: 404)\n",
      "\n",
      "Question: \n",
      "What is the main purpose of a computer's CPU?\n",
      "A) Store data permanently\n",
      "B) Process instructions and perform calculations\n",
      "C) Display images on the screen\n",
      "D) Connect to the internet\n",
      "\n",
      "Answer: Oops! Something went wrong: model \"llama2\" not found, try pulling it first (status code: 404)\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "# These classes tell the AI exactly what kind of answer we want\n",
    "class YesNoAnswer(BaseModel):\n",
    "    \"\"\"For questions that should be answered with Yes or No\"\"\"\n",
    "    answer: Literal[\"Yes\", \"No\"]\n",
    "\n",
    "class TrueFalseAnswer(BaseModel):\n",
    "    \"\"\"For statements that should be marked True or False\"\"\"\n",
    "    answer: bool\n",
    "\n",
    "class MultipleChoiceAnswer(BaseModel):\n",
    "    \"\"\"For questions with A, B, C, or D options\"\"\"\n",
    "    answer: Literal[\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def get_structured_answer(prompt: str, answer_type: BaseModel, model: str = \"llama2\"):\n",
    "    \"\"\"\n",
    "    Get a specific type of answer from the AI\n",
    "    \n",
    "    Args:\n",
    "        prompt: Your question\n",
    "        answer_type: What kind of answer you want (YesNoAnswer, TrueFalseAnswer, or MultipleChoiceAnswer)\n",
    "        model: Which AI model to use\n",
    "    \"\"\"\n",
    "    # Tell the AI what kind of answer we want\n",
    "    system_prompt = f\"\"\"\n",
    "    You must respond in valid JSON format.\n",
    "    Your answer must follow this structure:\n",
    "    {answer_type.model_json_schema()}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Ask the AI\n",
    "        response = chat(\n",
    "            model=model,\n",
    "            options={\"temperature\": 0.0},  # We want consistent answers here\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            format=\"json\"\n",
    "        )\n",
    "        \n",
    "        # Get and validate the answer\n",
    "        content = response[\"message\"][\"content\"]\n",
    "        validated_answer = answer_type.model_validate_json(content)\n",
    "        return validated_answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Oops! Something went wrong: {e}\"\n",
    "\n",
    "# Let's try some examples!\n",
    "\n",
    "# Yes/No question\n",
    "yes_no_q = \"Is Python a programming language?\"\n",
    "yes_no_answer = get_structured_answer(yes_no_q, YesNoAnswer)\n",
    "print(f\"\\nQuestion: {yes_no_q}\")\n",
    "print(f\"Answer: {yes_no_answer.answer if hasattr(yes_no_answer, 'answer') else yes_no_answer}\")\n",
    "\n",
    "# True/False statement\n",
    "true_false_q = \"The Earth is flat.\"\n",
    "true_false_answer = get_structured_answer(true_false_q, TrueFalseAnswer)\n",
    "print(f\"\\nStatement: {true_false_q}\")\n",
    "print(f\"Answer: {true_false_answer.answer if hasattr(true_false_answer, 'answer') else true_false_answer}\")\n",
    "\n",
    "# Multiple choice question\n",
    "multiple_choice_q = \"\"\"\n",
    "What is the main purpose of a computer's CPU?\n",
    "A) Store data permanently\n",
    "B) Process instructions and perform calculations\n",
    "C) Display images on the screen\n",
    "D) Connect to the internet\n",
    "\"\"\"\n",
    "mc_answer = get_structured_answer(multiple_choice_q, MultipleChoiceAnswer)\n",
    "print(f\"\\nQuestion: {multiple_choice_q}\")\n",
    "print(f\"Answer: {mc_answer.answer if hasattr(mc_answer, 'answer') else mc_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Chat with Memory: Having Real Conversations! üß†\n",
    "\n",
    "One of the coolest things about chatting with AI is that it can remember what you talked about earlier in the conversation. This is called \"chat history\" or \"context\".\n",
    "\n",
    "For example, if you ask about the weather and then ask \"Why is that?\", the AI will know you're asking about the weather!\n",
    "\n",
    "Let's try having a conversation where the AI remembers what we talked about:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "# This is our conversation history - it's like the AI's memory!\n",
    "messages = [\n",
    "    # First, we ask about the sky\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Why is the sky blue?',\n",
    "    },\n",
    "    # The AI's response about the sky\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \"The sky is blue because of the way the Earth's atmosphere scatters sunlight.\",\n",
    "    },\n",
    "    # Then we ask about weather\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'What is the weather in Tokyo?',\n",
    "    },\n",
    "    # The AI's response about Tokyo weather\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': 'The weather in Tokyo is typically warm and humid during the summer months, with temperatures often exceeding 30¬∞C (86¬∞F). The city experiences a rainy season from June to September, with heavy rainfall and occasional typhoons. Winter is mild, with temperatures rarely dropping below freezing.',\n",
    "    },\n",
    "]\n",
    "\n",
    "def display_chat_history():\n",
    "    \"\"\"Show the conversation so far\"\"\"\n",
    "    print(\"\\nüó®Ô∏è Chat History:\")\n",
    "    for msg in messages:\n",
    "        if msg['role'] == 'user':\n",
    "            print(f\"\\nüë§ You: {msg['content']}\")\n",
    "        else:\n",
    "            print(f\"ü§ñ AI: {msg['content']}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Show our chat history so far\n",
    "display_chat_history()\n",
    "\n",
    "# Let's have a conversation!\n",
    "print(\"Let's chat! (Type 'quit' to end the conversation)\\n\")\n",
    "\n",
    "while True:\n",
    "    # Get what the user wants to say\n",
    "    user_input = input('üë§ You: ')\n",
    "    \n",
    "    # Check if the user wants to quit\n",
    "    if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "        print(\"\\nüëã Thanks for chatting!\")\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        # Send the message and all previous history to the AI\n",
    "        response = chat(\n",
    "            'llama3.2',\n",
    "            messages=[*messages, {'role': 'user', 'content': user_input}],\n",
    "        )\n",
    "        \n",
    "        # Show the AI's response\n",
    "        print(f\"\\nü§ñ AI: {response.message.content}\\n\")\n",
    "        \n",
    "        # Add both the user's message and AI's response to the history\n",
    "        messages.extend([\n",
    "            {'role': 'user', 'content': user_input},\n",
    "            {'role': 'assistant', 'content': response.message.content},\n",
    "        ])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Oops! Something went wrong: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### How Chat History Works ü§î\n",
    "\n",
    "When we chat with the AI, we're doing something pretty clever:\n",
    "\n",
    "1. We keep a list of all messages (both yours and the AI's) in the `messages` list\n",
    "2. Each message has:\n",
    "   - A `role`: either 'user' (you) or 'assistant' (the AI)\n",
    "   - `content`: what was said\n",
    "\n",
    "3. Every time we send a new message:\n",
    "   - We include ALL previous messages\n",
    "   - This helps the AI understand the context\n",
    "   - The AI can refer back to what was said before\n",
    "\n",
    "This is why the AI can understand follow-up questions like:\n",
    "- \"Can you explain that more?\"\n",
    "- \"Why is that?\"\n",
    "- \"What about in winter?\"\n",
    "\n",
    "Try asking some follow-up questions in the chat above to see how the AI uses the conversation history! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Time! üéÆ\n",
    "\n",
    "Now it's your turn to try! Here are some fun exercises you can try:\n",
    "\n",
    "1. **Simple Chat**:\n",
    "   - Ask the AI to explain your favorite subject in a fun way\n",
    "   - Ask for a short story about a topic you're interested in\n",
    "   - Have the AI explain a difficult concept using an analogy\n",
    "\n",
    "2. **Yes/No Questions**:\n",
    "   - Is the speed of light faster than the speed of sound?\n",
    "   - Are dolphins mammals?\n",
    "   - Can humans breathe underwater naturally?\n",
    "\n",
    "3. **Multiple Choice**:\n",
    "   - Create your own quiz about a topic you're studying\n",
    "   - Ask the AI to choose the best answer\n",
    "   - Save the conversation to review later\n",
    "\n",
    "Remember:\n",
    "- You can adjust the `temperature` to make the AI more creative (higher) or more focused (lower)\n",
    "- Always save interesting conversations using our `save_chat` function\n",
    "- Try different AI models if they're available on your computer\n",
    "\n",
    "Have fun chatting with AI! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
