{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with Memory: Having Real Conversations! 🧠\n",
    "\n",
    "One of the coolest things about chatting with AI is that it can remember what you talked about earlier in the conversation. This is called \"chat history\" or \"context\".\n",
    "\n",
    "For example, if you ask about the weather and then ask \"Why is that?\", the AI will know you're asking about the weather!\n",
    "\n",
    "Let's try having a conversation where the AI remembers what we talked about:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "import psutil\n",
    "from ollama import chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Ollama Server 🖥️\n",
    "\n",
    "Before we can use any Ollama models, we need to make sure the Ollama server is running. Let's create a helper function that will:\n",
    "1. Check if an Ollama server is already running\n",
    "2. Start a new server only if needed\n",
    "3. Keep track of the server process\n",
    "\n",
    "This way, we won't accidentally start multiple servers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_port_in_use(port: int = 11434) -> bool:\n",
    "    \"\"\"Check if the Ollama port is already in use\"\"\"\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        return s.connect_ex(('localhost', port)) == 0\n",
    "\n",
    "def is_ollama_running() -> bool:\n",
    "    \"\"\"Check if Ollama process is running\"\"\"\n",
    "    for proc in psutil.process_iter(['name']):\n",
    "        try:\n",
    "            if proc.info['name'] == 'ollama':\n",
    "                return True\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def ensure_ollama_server():\n",
    "    \"\"\"\n",
    "    Ensures that an Ollama server is running.\n",
    "    Returns the process object if a new server was started, None if server was already running.\n",
    "    \"\"\"\n",
    "    # First check if the server is already running\n",
    "    if is_port_in_use() or is_ollama_running():\n",
    "        print(\"✅ Ollama server is already running!\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Start the Ollama server\n",
    "        print(\"🚀 Starting Ollama server...\")\n",
    "        proc = subprocess.Popen(\n",
    "            [\"ollama\", \"serve\"],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            start_new_session=True\n",
    "        )\n",
    "        \n",
    "        # Wait for the server to start (max 10 seconds)\n",
    "        for _ in range(10):\n",
    "            if is_port_in_use():\n",
    "                print(f\"✅ Ollama server started successfully (PID: {proc.pid})\")\n",
    "                return proc\n",
    "            time.sleep(1)\n",
    "            \n",
    "        raise TimeoutError(\"Server didn't start within 10 seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error starting Ollama server: {e}\")\n",
    "        if 'proc' in locals():\n",
    "            proc.terminate()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ollama server is already running!\n"
     ]
    }
   ],
   "source": [
    "# Start the server if it's not already running\n",
    "ollama_process = ensure_ollama_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Chat with Memory: Having Real Conversations! 🧠\n",
    "\n",
    "One of the coolest things about chatting with AI is that it can remember what you talked about earlier in the conversation. This is called \"chat history\" or \"context\".\n",
    "\n",
    "For example, if you ask about the weather and then ask \"Why is that?\", the AI will know you're asking about the weather!\n",
    "\n",
    "Let's try having a conversation where the AI remembers what we talked about:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our conversation history - it's like the AI's memory!\n",
    "messages = [\n",
    "    # First, we ask about the sky\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Why is the sky blue?',\n",
    "    },\n",
    "    # The AI's response about the sky\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \"The sky is blue because of the way the Earth's atmosphere scatters sunlight.\",\n",
    "    },\n",
    "    # Then we ask about weather\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'What is the weather in Tokyo?',\n",
    "    },\n",
    "    # The AI's response about Tokyo weather\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': 'The weather in Tokyo is typically warm and humid during the summer months, with temperatures often exceeding 30°C (86°F). The city experiences a rainy season from June to September, with heavy rainfall and occasional typhoons. Winter is mild, with temperatures rarely dropping below freezing.',\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_chat_history():\n",
    "    \"\"\"Show the conversation so far\"\"\"\n",
    "    print(\"\\n🗨️ Chat History:\")\n",
    "    for msg in messages:\n",
    "        if msg['role'] == 'user':\n",
    "            print(f\"\\n👤 You: {msg['content']}\")\n",
    "        else:\n",
    "            print(f\"🤖 AI: {msg['content']}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗨️ Chat History:\n",
      "\n",
      "👤 You: Why is the sky blue?\n",
      "🤖 AI: The sky is blue because of the way the Earth's atmosphere scatters sunlight.\n",
      "\n",
      "👤 You: What is the weather in Tokyo?\n",
      "🤖 AI: The weather in Tokyo is typically warm and humid during the summer months, with temperatures often exceeding 30°C (86°F). The city experiences a rainy season from June to September, with heavy rainfall and occasional typhoons. Winter is mild, with temperatures rarely dropping below freezing.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show our chat history so far\n",
    "display_chat_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (Type 'quit' to end the conversation)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You:  why is it like that\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 AI: <think>\n",
      "Okay, the user is asking why the sky is blue, and they also mentioned the weather in Tokyo. Let me start by addressing the sky question first. I should explain the reason for the blue sky, which is due to atmospheric scattering. Then, for Tokyo, I need to cover the current weather and maybe mention the seasonal patterns.\n",
      "\n",
      "Wait, the user is using \"why is it like that\" as a question, so maybe they're trying to get a more detailed explanation. I should make sure my answers are clear and concise. Also, check if there's any specific information needed about Tokyo's weather that the user is interested in. Maybe they want to know about the rainy season or the temperature ranges. I should structure the response to cover both topics, ensuring that each explanation is explained clearly and logically.\n",
      "</think>\n",
      "\n",
      "The sky appears blue because sunlight is scattered by the Earth's atmosphere. Specifically, water molecules and other molecules in the air refract and disperse the sunlight, creating a blue hue.  \n",
      "\n",
      "For Tokyo, the weather is typically warm and humid during summer (March to May), with temperatures often exceeding 30°C (86°F). In the rainy season, June to September, the city experiences heavy rainfall, typhoons, and occasional snowfall, while winter is mild with temperatures rarely dropping below freezing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "👋 Thanks for chatting!\n"
     ]
    }
   ],
   "source": [
    "# Let's have a conversation!\n",
    "print(\"Let's chat! (Type 'quit' to end the conversation)\\n\")\n",
    "while True:\n",
    "    # Get what the user wants to say\n",
    "    user_input = input('👤 You: ')\n",
    "    \n",
    "    # Check if the user wants to quit\n",
    "    if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "        print(\"\\n👋 Thanks for chatting!\")\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        # Send the message and all previous history to the AI\n",
    "        response = chat(\n",
    "            'qwen3:0.6b',\n",
    "            messages=[*messages, {'role': 'user', 'content': user_input}],\n",
    "        )\n",
    "        \n",
    "        # Show the AI's response\n",
    "        print(f\"\\n🤖 AI: {response.message.content}\\n\")\n",
    "        \n",
    "        # Add both the user's message and AI's response to the history\n",
    "        messages.extend([\n",
    "            {'role': 'user', 'content': user_input},\n",
    "            {'role': 'assistant', 'content': response.message.content},\n",
    "        ])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Oops! Something went wrong: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### How Chat History Works 🤔\n",
    "\n",
    "When we chat with the AI, we're doing something pretty clever:\n",
    "\n",
    "1. We keep a list of all messages (both yours and the AI's) in the `messages` list\n",
    "2. Each message has:\n",
    "   - A `role`: either 'user' (you) or 'assistant' (the AI)\n",
    "   - `content`: what was said\n",
    "\n",
    "3. Every time we send a new message:\n",
    "   - We include ALL previous messages\n",
    "   - This helps the AI understand the context\n",
    "   - The AI can refer back to what was said before\n",
    "\n",
    "This is why the AI can understand follow-up questions like:\n",
    "- \"Can you explain that more?\"\n",
    "- \"Why is that?\"\n",
    "- \"What about in winter?\"\n",
    "\n",
    "Try asking some follow-up questions in the chat above to see how the AI uses the conversation history! 🚀\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
