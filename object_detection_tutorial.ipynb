{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Object Detection with Vision Language Models Tutorial ðŸ‘ï¸ðŸ¤–\n",
        "\n",
        "Welcome to an exciting journey into computer vision using AI! In this tutorial, we'll learn how to build an intelligent object detection system that can identify and analyze objects in images.\n",
        "\n",
        "## What You'll Learn ðŸŽ¯\n",
        "\n",
        "In this comprehensive tutorial, you'll discover:\n",
        "\n",
        "1. **ðŸ‘ï¸ Vision Language Models**: How AI can \"see\" and understand images\n",
        "2. **ðŸ” Object Detection**: Identifying and counting objects in photos\n",
        "3. **ðŸ“Š Structured Outputs**: Getting organized data from AI responses\n",
        "4. **ðŸŽ¨ Data Visualization**: Creating charts and graphs from detection results\n",
        "5. **ðŸŒ Web Applications**: Building interactive interfaces with Streamlit\n",
        "6. **ðŸ—ï¸ System Architecture**: Organizing code into modular components\n",
        "\n",
        "Let's dive into the fascinating world of AI vision! ðŸš€\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Prerequisites ðŸ› ï¸\n",
        "\n",
        "Before we start building our object detection system, let's make sure we have everything we need:\n",
        "\n",
        "### What We Need:\n",
        "- **Ollama** running on your computer with a vision model\n",
        "- **Python packages**: ollama, pydantic, streamlit, pillow, matplotlib\n",
        "- **Vision Model**: llama3.2-vision or similar\n",
        "\n",
        "Let's install the required packages and set up our imports:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install ollama pydantic streamlit pillow pandas matplotlib\n",
        "\n",
        "# Import all necessary libraries\n",
        "import ollama\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"âœ… All packages installed and imported!\")\n",
        "print(\"ðŸŽ¯ Ready to build our object detection system!\")\n",
        "\n",
        "# Check if we have access to Ollama\n",
        "try:\n",
        "    models = ollama.list()\n",
        "    print(f\"\\\\nðŸ¤– Available Ollama models: {len(models['models'])}\")\n",
        "    \n",
        "    # Look for vision models\n",
        "    vision_models = [m for m in models['models'] if 'vision' in m['model'].lower()]\n",
        "    if vision_models:\n",
        "        print(f\"ðŸ‘ï¸ Vision models found: {[m['model'] for m in vision_models]}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No vision models found. You may need to install one:\")\n",
        "        print(\"   Run: ollama pull llama3.2-vision\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error connecting to Ollama: {e}\")\n",
        "    print(\"Make sure Ollama is running!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Part 1: Understanding Vision Language Models ðŸ‘ï¸\n",
        "\n",
        "Vision Language Models (VLMs) are AI systems that can both see images and understand language. They combine:\n",
        "\n",
        "- **Computer Vision**: The ability to process and understand images\n",
        "- **Natural Language Processing**: The ability to understand and generate text\n",
        "- **Multimodal Understanding**: Connecting visual and textual information\n",
        "\n",
        "### How VLMs Work:\n",
        "1. **Image Encoding**: Convert pixels into numerical representations\n",
        "2. **Feature Extraction**: Identify patterns, shapes, colors, and objects\n",
        "3. **Language Integration**: Connect visual features with text descriptions\n",
        "4. **Response Generation**: Produce structured or natural language outputs\n",
        "\n",
        "Let's see this in action!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's test basic vision capabilities\n",
        "def test_vision_model(image_path: str, model: str = \"llama3.2-vision\"):\n",
        "    \"\"\"\n",
        "    Test if our vision model can see and describe an image.\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        model: Name of the vision model to use\n",
        "    \n",
        "    Returns:\n",
        "        The model's description of the image\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = ollama.chat(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\n",
        "                    'role': 'user',\n",
        "                    'content': 'Describe what you see in this image in detail.',\n",
        "                    'images': [image_path]\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "        return response['message']['content']\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Create a simple test image (if you don't have one handy)\n",
        "def create_test_image():\n",
        "    \"\"\"Create a simple test image with basic shapes\"\"\"\n",
        "    from PIL import Image, ImageDraw\n",
        "    \n",
        "    # Create a simple image with shapes\n",
        "    img = Image.new('RGB', (400, 300), color='lightblue')\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    # Draw some shapes\n",
        "    draw.rectangle([50, 50, 150, 150], fill='red', outline='black', width=2)\n",
        "    draw.ellipse([200, 100, 300, 200], fill='yellow', outline='black', width=2)\n",
        "    draw.polygon([(320, 50), (370, 50), (345, 100)], fill='green', outline='black', width=2)\n",
        "    \n",
        "    # Save the test image\n",
        "    test_path = \"test_shapes.png\"\n",
        "    img.save(test_path)\n",
        "    print(f\"âœ… Created test image: {test_path}\")\n",
        "    return test_path\n",
        "\n",
        "# Create and test with a simple image\n",
        "print(\"ðŸŽ¨ Creating a test image with basic shapes...\")\n",
        "test_image_path = create_test_image()\n",
        "\n",
        "# Display the test image\n",
        "try:\n",
        "    test_img = Image.open(test_image_path)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(test_img)\n",
        "    plt.title(\"Our Test Image\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\\\nðŸ¤– Testing vision model...\")\n",
        "    description = test_vision_model(test_image_path)\n",
        "    print(f\"ðŸ‘ï¸ AI Description: {description}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error testing vision: {e}\")\n",
        "    print(\"Make sure you have a vision model installed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Part 2: Structured Object Detection ðŸ“Š\n",
        "\n",
        "While getting natural language descriptions is great, for applications we often need structured data. Let's build a system that returns organized information about detected objects.\n",
        "\n",
        "We'll use **Pydantic** to define data structures that ensure our AI returns consistent, well-formatted results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define data structures for our object detection results\n",
        "class Object(BaseModel):\n",
        "    \"\"\"\n",
        "    Represents a detected object in an image.\n",
        "    \n",
        "    This structure ensures that for each object we detect, we get:\n",
        "    - A clear name/label\n",
        "    - The colors we can see\n",
        "    - How many instances are present\n",
        "    \"\"\"\n",
        "    name: str                # What is the object? (e.g., \"car\", \"person\", \"tree\")\n",
        "    color: List[str]         # What colors do we see? (e.g., [\"red\", \"blue\"])\n",
        "    count: int               # How many are there? (e.g., 3)\n",
        "\n",
        "class ObjectDetectionResponse(BaseModel):\n",
        "    \"\"\"\n",
        "    The complete response from our object detection system.\n",
        "    \n",
        "    This contains a list of all detected objects.\n",
        "    \"\"\"\n",
        "    objects: List[Object]\n",
        "\n",
        "def detect_objects(image_path: str, model: str = \"llama3.2-vision\") -> str:\n",
        "    \"\"\"\n",
        "    Detects objects in an image using a vision language model.\n",
        "    \n",
        "    This is the core function that:\n",
        "    1. Sends an image to the AI model\n",
        "    2. Asks for structured object detection\n",
        "    3. Returns organized JSON data\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the image file to analyze\n",
        "        model: Name of the vision model to use\n",
        "    \n",
        "    Returns:\n",
        "        JSON string containing detected objects with their properties\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Send the image and structured prompt to the AI model\n",
        "        response = ollama.chat(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\n",
        "                    'role': 'user',\n",
        "                    'content': \\\"\\\"\\\"Your task is to perform object detection on the image and return a structured output in JSON format. For each detected object, include the following attributes:\n",
        "                    Name: The name of the detected object (e.g., 'cat', 'car', 'person').\n",
        "                    Count: The total number of detected instances of this object type in the image.\n",
        "                    Color: The dominant color or primary colors of the object.\n",
        "                    \n",
        "                    Return the results as a JSON object with an 'objects' array.\\\"\\\"\\\",\n",
        "                    'images': [image_path]\n",
        "                }\n",
        "            ],\n",
        "            format=\"json\",          # This tells Ollama to return JSON\n",
        "            options={'temperature': 0}  # Low temperature for consistent results\n",
        "        )\n",
        "        return response['message']['content']\n",
        "    \n",
        "    except Exception as e:\n",
        "        # Return error information in a structured format\n",
        "        error_response = {\n",
        "            \"objects\": [],\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "        return json.dumps(error_response)\n",
        "\n",
        "# Test our structured detection\n",
        "print(\"ðŸ” Testing structured object detection...\")\n",
        "\n",
        "try:\n",
        "    # Use our test image from before\n",
        "    detection_result = detect_objects(test_image_path)\n",
        "    print(\"\\\\nðŸ“Š Raw JSON Response:\")\n",
        "    print(detection_result)\n",
        "    \n",
        "    # Parse the JSON to make it more readable\n",
        "    try:\n",
        "        parsed_result = json.loads(detection_result)\n",
        "        print(\"\\\\nâœ¨ Parsed Results:\")\n",
        "        \n",
        "        if \"objects\" in parsed_result:\n",
        "            for i, obj in enumerate(parsed_result[\"objects\"], 1):\n",
        "                print(f\"  {i}. Object: {obj.get('name', 'Unknown')}\")\n",
        "                print(f\"     Count: {obj.get('count', 0)}\")\n",
        "                print(f\"     Colors: {', '.join(obj.get('color', []))}\")\n",
        "                print()\n",
        "        else:\n",
        "            print(\"No objects detected or error in response.\")\n",
        "            \n",
        "    except json.JSONDecodeError:\n",
        "        print(\"âš ï¸ Response was not valid JSON\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error during detection: {e}\")\n",
        "\n",
        "print(\"\\\\nðŸ’¡ The power of structured outputs:\")\n",
        "print(\"âœ… Consistent data format\")\n",
        "print(\"âœ… Easy to process programmatically\") \n",
        "print(\"âœ… Can be stored in databases\")\n",
        "print(\"âœ… Perfect for building applications\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Part 3: Data Visualization ðŸ“ˆ\n",
        "\n",
        "Now that we can detect objects, let's create beautiful visualizations to display our results! Data visualization helps us understand patterns and makes our results more engaging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_object_count_chart(detection_data: str) -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Create a bar chart showing the count of each detected object type.\n",
        "    \n",
        "    Args:\n",
        "        detection_data: JSON string from our object detection function\n",
        "        \n",
        "    Returns:\n",
        "        A matplotlib Figure object containing the chart\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the JSON data\n",
        "        data = json.loads(detection_data)\n",
        "        \n",
        "        if \"objects\" not in data or not data[\"objects\"]:\n",
        "            print(\"No objects found to visualize\")\n",
        "            return None\n",
        "            \n",
        "        # Extract object names and counts\n",
        "        objects = data[\"objects\"]\n",
        "        names = [obj.get(\"name\", \"Unknown\") for obj in objects]\n",
        "        counts = [obj.get(\"count\", 0) for obj in objects]\n",
        "        \n",
        "        # Create the bar chart\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        bars = ax.bar(names, counts, color='skyblue', edgecolor='navy', linewidth=1.2)\n",
        "        \n",
        "        # Customize the chart\n",
        "        ax.set_xlabel('Object Type', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Detected Objects Count', fontsize=14, fontweight='bold')\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # Add count labels on top of bars\n",
        "        for bar, count in zip(bars, counts):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
        "                   f'{count}', ha='center', va='bottom', fontweight='bold')\n",
        "        \n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error creating chart: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_color_distribution_chart(detection_data: str) -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Create a pie chart showing the distribution of colors in detected objects.\n",
        "    \n",
        "    Args:\n",
        "        detection_data: JSON string from our object detection function\n",
        "        \n",
        "    Returns:\n",
        "        A matplotlib Figure object containing the pie chart\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the JSON data\n",
        "        data = json.loads(detection_data)\n",
        "        \n",
        "        if \"objects\" not in data or not data[\"objects\"]:\n",
        "            print(\"No objects found to visualize\")\n",
        "            return None\n",
        "            \n",
        "        # Count all colors mentioned\n",
        "        color_counts = {}\n",
        "        for obj in data[\"objects\"]:\n",
        "            colors = obj.get(\"color\", [])\n",
        "            count = obj.get(\"count\", 1)\n",
        "            \n",
        "            for color in colors:\n",
        "                color_lower = color.lower()\n",
        "                color_counts[color_lower] = color_counts.get(color_lower, 0) + count\n",
        "        \n",
        "        if not color_counts:\n",
        "            print(\"No colors found to visualize\")\n",
        "            return None\n",
        "            \n",
        "        # Create the pie chart\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        colors_list = list(color_counts.keys())\n",
        "        counts_list = list(color_counts.values())\n",
        "        \n",
        "        # Use actual colors where possible\n",
        "        color_map = {\n",
        "            'red': '#FF6B6B', 'blue': '#4ECDC4', 'green': '#45B7D1',\n",
        "            'yellow': '#FFA07A', 'orange': '#FF8C42', 'purple': '#9B59B6',\n",
        "            'pink': '#FF69B4', 'brown': '#8B4513', 'black': '#2C3E50',\n",
        "            'white': '#ECF0F1', 'gray': '#95A5A6', 'grey': '#95A5A6'\n",
        "        }\n",
        "        \n",
        "        pie_colors = [color_map.get(color, '#BDC3C7') for color in colors_list]\n",
        "        \n",
        "        wedges, texts, autotexts = ax.pie(counts_list, labels=colors_list, colors=pie_colors,\n",
        "                                         autopct='%1.1f%%', startangle=90)\n",
        "        \n",
        "        ax.set_title('Color Distribution in Detected Objects', fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error creating color chart: {e}\")\n",
        "        return None\n",
        "\n",
        "def display_detection_results(detection_data: str):\n",
        "    \"\"\"\n",
        "    Display comprehensive results from object detection including charts.\n",
        "    \n",
        "    This function creates a complete analysis dashboard showing:\n",
        "    - Summary statistics\n",
        "    - Object count chart\n",
        "    - Color distribution chart\n",
        "    - Raw data table\n",
        "    \"\"\"\n",
        "    print(\"\\\\nðŸ“Š OBJECT DETECTION RESULTS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Parse the detection data\n",
        "        data = json.loads(detection_data)\n",
        "        \n",
        "        if \"error\" in data:\n",
        "            print(f\"âŒ Error in detection: {data['error']}\")\n",
        "            return\n",
        "            \n",
        "        if \"objects\" not in data or not data[\"objects\"]:\n",
        "            print(\"ðŸ” No objects detected in the image.\")\n",
        "            return\n",
        "            \n",
        "        objects = data[\"objects\"]\n",
        "        \n",
        "        # Summary statistics\n",
        "        total_objects = sum(obj.get(\"count\", 0) for obj in objects)\n",
        "        unique_types = len(objects)\n",
        "        \n",
        "        print(f\"ðŸ“ˆ Summary:\")\n",
        "        print(f\"   Total Objects: {total_objects}\")\n",
        "        print(f\"   Unique Types: {unique_types}\")\n",
        "        print()\n",
        "        \n",
        "        # Detailed object list\n",
        "        print(\"ðŸ” Detected Objects:\")\n",
        "        for i, obj in enumerate(objects, 1):\n",
        "            name = obj.get(\"name\", \"Unknown\")\n",
        "            count = obj.get(\"count\", 0)\n",
        "            colors = obj.get(\"color\", [])\n",
        "            print(f\"   {i}. {name.title()}\")\n",
        "            print(f\"      Count: {count}\")\n",
        "            print(f\"      Colors: {', '.join(colors) if colors else 'Not specified'}\")\n",
        "            print()\n",
        "        \n",
        "        # Create and display charts\n",
        "        print(\"ðŸ“Š Creating visualizations...\")\n",
        "        \n",
        "        # Object count chart\n",
        "        count_fig = create_object_count_chart(detection_data)\n",
        "        if count_fig:\n",
        "            plt.figure(count_fig.number)\n",
        "            plt.show()\n",
        "        \n",
        "        # Color distribution chart  \n",
        "        color_fig = create_color_distribution_chart(detection_data)\n",
        "        if color_fig:\n",
        "            plt.figure(color_fig.number)\n",
        "            plt.show()\n",
        "            \n",
        "    except json.JSONDecodeError:\n",
        "        print(\"âŒ Invalid JSON data received\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error displaying results: {e}\")\n",
        "\n",
        "# Test our visualization functions\n",
        "print(\"ðŸŽ¨ Testing our visualization system...\")\n",
        "\n",
        "# Create some sample detection data for testing\n",
        "sample_data = {\n",
        "    \"objects\": [\n",
        "        {\"name\": \"rectangle\", \"count\": 1, \"color\": [\"red\"]},\n",
        "        {\"name\": \"circle\", \"count\": 1, \"color\": [\"yellow\"]},\n",
        "        {\"name\": \"triangle\", \"count\": 1, \"color\": [\"green\"]}\n",
        "    ]\n",
        "}\n",
        "\n",
        "sample_json = json.dumps(sample_data)\n",
        "print(\"\\\\nðŸ§ª Using sample data for demonstration:\")\n",
        "\n",
        "# Display the results\n",
        "display_detection_results(sample_json)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Part 4: Building a Web Application ðŸŒ\n",
        "\n",
        "Now let's put everything together into a user-friendly web application using Streamlit! This will allow users to upload images and get instant object detection results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete Streamlit Application Code\n",
        "# This is the code that would go in your app.py file\n",
        "\n",
        "streamlit_app_code = '''\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "from detector import detect_objects\n",
        "from visualize import display_detection_results\n",
        "\n",
        "# --- Page Configuration ---\n",
        "st.set_page_config(\n",
        "    page_title=\"Object Detection\",\n",
        "    page_icon=\"ðŸ¤–\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# --- App Title ---\n",
        "st.title(\"ðŸ¤– Object Detection\")\n",
        "st.write(\"Upload an image to detect objects using a local vision model.\")\n",
        "\n",
        "# --- Sidebar with Information ---\n",
        "st.sidebar.title(\"â„¹ï¸ About\")\n",
        "st.sidebar.write(\"\"\"\n",
        "This application uses AI vision models to:\n",
        "- ðŸ” Detect objects in images\n",
        "- ðŸ“Š Count instances of each object\n",
        "- ðŸŽ¨ Identify colors\n",
        "- ðŸ“ˆ Create visualizations\n",
        "\n",
        "**How to use:**\n",
        "1. Upload an image (JPG, PNG, JPEG)\n",
        "2. Click \"Detect Objects\"\n",
        "3. View the results and charts!\n",
        "\"\"\")\n",
        "\n",
        "# --- File Uploader ---\n",
        "uploaded_file = st.file_uploader(\n",
        "    \"Choose an image...\", \n",
        "    type=[\"jpg\", \"png\", \"jpeg\"]\n",
        ")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Create two columns for layout\n",
        "    col1, col2 = st.columns([1, 1])\n",
        "    \n",
        "    with col1:\n",
        "        # Display the uploaded image\n",
        "        image = Image.open(uploaded_file)\n",
        "        st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "    \n",
        "    with col2:\n",
        "        # Show image information\n",
        "        st.write(\"**Image Information:**\")\n",
        "        st.write(f\"- **Filename:** {uploaded_file.name}\")\n",
        "        st.write(f\"- **Size:** {image.size}\")\n",
        "        st.write(f\"- **Format:** {image.format}\")\n",
        "        st.write(f\"- **Mode:** {image.mode}\")\n",
        "\n",
        "    # A button to trigger the analysis\n",
        "    if st.button(\"ðŸ” Detect Objects\", type=\"primary\"):\n",
        "        with st.spinner(\"Detecting objects... This may take a moment.\"):\n",
        "            try:\n",
        "                # Save the uploaded file temporarily to pass its path to the model\n",
        "                temp_dir = \"temp\"\n",
        "                if not os.path.exists(temp_dir):\n",
        "                    os.makedirs(temp_dir)\n",
        "                \n",
        "                file_path = os.path.join(temp_dir, uploaded_file.name)\n",
        "                with open(file_path, \"wb\") as f:\n",
        "                    f.write(uploaded_file.getbuffer())\n",
        "\n",
        "                # Call the object detection function\n",
        "                detection_data = detect_objects(file_path)\n",
        "                \n",
        "                # Parse and display results\n",
        "                try:\n",
        "                    data = json.loads(detection_data)\n",
        "                    \n",
        "                    if \"error\" in data:\n",
        "                        st.error(f\"Detection error: {data['error']}\")\n",
        "                    elif \"objects\" in data and data[\"objects\"]:\n",
        "                        st.success(\"âœ… Objects detected successfully!\")\n",
        "                        \n",
        "                        # Display summary\n",
        "                        objects = data[\"objects\"]\n",
        "                        total_objects = sum(obj.get(\"count\", 0) for obj in objects)\n",
        "                        \n",
        "                        col1, col2, col3 = st.columns(3)\n",
        "                        with col1:\n",
        "                            st.metric(\"Total Objects\", total_objects)\n",
        "                        with col2:\n",
        "                            st.metric(\"Unique Types\", len(objects))\n",
        "                        with col3:\n",
        "                            all_colors = []\n",
        "                            for obj in objects:\n",
        "                                all_colors.extend(obj.get(\"color\", []))\n",
        "                            st.metric(\"Colors Found\", len(set(all_colors)))\n",
        "                        \n",
        "                        # Display detailed results\n",
        "                        st.subheader(\"ðŸ“‹ Detected Objects\")\n",
        "                        for i, obj in enumerate(objects, 1):\n",
        "                            with st.expander(f\"{obj.get('name', 'Unknown').title()} ({obj.get('count', 0)} found)\"):\n",
        "                                st.write(f\"**Count:** {obj.get('count', 0)}\")\n",
        "                                colors = obj.get('color', [])\n",
        "                                if colors:\n",
        "                                    st.write(f\"**Colors:** {', '.join(colors)}\")\n",
        "                                else:\n",
        "                                    st.write(\"**Colors:** Not specified\")\n",
        "                        \n",
        "                        # Create visualizations (simplified for Streamlit)\n",
        "                        st.subheader(\"ðŸ“Š Visualizations\")\n",
        "                        \n",
        "                        # Object counts as a simple chart\n",
        "                        chart_data = {obj.get(\"name\", \"Unknown\"): obj.get(\"count\", 0) for obj in objects}\n",
        "                        st.bar_chart(chart_data)\n",
        "                        \n",
        "                    else:\n",
        "                        st.info(\"ðŸ” No objects detected in the image.\")\n",
        "                        \n",
        "                except json.JSONDecodeError:\n",
        "                    st.error(\"âŒ Invalid response from detection model\")\n",
        "\n",
        "                # Clean up the temporary file\n",
        "                if os.path.exists(file_path):\n",
        "                    os.remove(file_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"An error occurred during analysis: {e}\")\n",
        "\n",
        "else:\n",
        "    st.info(\"ðŸ‘† Please upload an image file to get started.\")\n",
        "    \n",
        "    # Show example images or instructions\n",
        "    st.subheader(\"ðŸ’¡ Tips for Better Results\")\n",
        "    st.write(\"\"\"\n",
        "    - Use clear, well-lit images\n",
        "    - Ensure objects are clearly visible\n",
        "    - Try different types of images (indoor, outdoor, people, animals, etc.)\n",
        "    - The AI works best with common objects it has been trained on\n",
        "    \"\"\")\n",
        "'''\n",
        "\n",
        "print(\"ðŸŒ Complete Streamlit Application Code:\")\n",
        "print(\"=\" * 50)\n",
        "print(\"This code creates a full web application with:\")\n",
        "print(\"âœ… File upload interface\")\n",
        "print(\"âœ… Image display and information\")\n",
        "print(\"âœ… Object detection processing\")\n",
        "print(\"âœ… Results visualization\")\n",
        "print(\"âœ… Error handling\")\n",
        "print(\"âœ… User-friendly interface\")\n",
        "\n",
        "print(\"\\\\nðŸ’¡ To run this as a web app:\")\n",
        "print(\"1. Save the code above to a file called 'app.py'\")\n",
        "print(\"2. Make sure you have the detector.py and visualize.py modules\")\n",
        "print(\"3. Run: streamlit run app.py\")\n",
        "print(\"4. Open your browser to the provided URL\")\n",
        "\n",
        "print(\"\\\\nðŸŽ¯ Key Features of Our Web App:\")\n",
        "features = [\n",
        "    \"ðŸ“¤ Drag-and-drop file upload\",\n",
        "    \"ðŸ–¼ï¸ Image preview and metadata\",\n",
        "    \"ðŸ” Real-time object detection\",\n",
        "    \"ðŸ“Š Interactive charts and metrics\",\n",
        "    \"ðŸ“± Responsive design\",\n",
        "    \"âš ï¸ Comprehensive error handling\",\n",
        "    \"ðŸ’¡ User guidance and tips\"\n",
        "]\n",
        "\n",
        "for feature in features:\n",
        "    print(f\"  {feature}\")\n",
        "\n",
        "# Create a simple demo function for notebook use\n",
        "def analyze_image_simple(image_path: str):\n",
        "    \"\"\"\n",
        "    Simplified version of our detection system for notebook use.\n",
        "    \"\"\"\n",
        "    print(f\"ðŸ” Analyzing image: {os.path.basename(image_path)}\")\n",
        "    \n",
        "    try:\n",
        "        # Run detection\n",
        "        result = detect_objects(image_path)\n",
        "        \n",
        "        # Display results in notebook-friendly format\n",
        "        display_detection_results(result)\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"\\\\nðŸ§ª You can test the detection system in this notebook using:\")\n",
        "print(\"analyze_image_simple('path_to_your_image.jpg')\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Part 5: Hands-On Experiments ðŸ§ª\n",
        "\n",
        "Now it's time to experiment! Let's try different approaches and see how our object detection system performs with various types of images and scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's create different test scenarios to explore our system\n",
        "\n",
        "def create_complex_test_image():\n",
        "    \"\"\"Create a more complex test image with multiple objects\"\"\"\n",
        "    from PIL import Image, ImageDraw, ImageFont\n",
        "    \n",
        "    # Create a larger, more complex image\n",
        "    img = Image.new('RGB', (600, 400), color='lightgray')\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    # Draw various objects\n",
        "    # Houses\n",
        "    draw.rectangle([50, 200, 120, 280], fill='brown', outline='black', width=2)\n",
        "    draw.polygon([(35, 200), (85, 150), (135, 200)], fill='red', outline='black', width=2)\n",
        "    \n",
        "    draw.rectangle([200, 220, 270, 300], fill='yellow', outline='black', width=2)\n",
        "    draw.polygon([(185, 220), (235, 170), (285, 220)], fill='blue', outline='black', width=2)\n",
        "    \n",
        "    # Trees\n",
        "    draw.rectangle([150, 250, 170, 320], fill='brown')  # trunk\n",
        "    draw.ellipse([130, 200, 190, 260], fill='green', outline='darkgreen', width=2)\n",
        "    \n",
        "    draw.rectangle([350, 240, 370, 310], fill='brown')  # trunk\n",
        "    draw.ellipse([330, 190, 390, 250], fill='green', outline='darkgreen', width=2)\n",
        "    \n",
        "    # Cars\n",
        "    draw.rectangle([400, 280, 480, 320], fill='red', outline='black', width=2)\n",
        "    draw.ellipse([410, 310, 430, 330], fill='black')  # wheel\n",
        "    draw.ellipse([460, 310, 480, 330], fill='black')  # wheel\n",
        "    \n",
        "    draw.rectangle([500, 270, 570, 310], fill='blue', outline='black', width=2)\n",
        "    draw.ellipse([510, 300, 530, 320], fill='black')  # wheel\n",
        "    draw.ellipse([550, 300, 570, 320], fill='black')  # wheel\n",
        "    \n",
        "    # Sun\n",
        "    draw.ellipse([500, 50, 550, 100], fill='yellow', outline='orange', width=3)\n",
        "    \n",
        "    # Clouds\n",
        "    draw.ellipse([100, 80, 160, 120], fill='white', outline='lightgray')\n",
        "    draw.ellipse([120, 70, 180, 110], fill='white', outline='lightgray')\n",
        "    draw.ellipse([300, 60, 380, 110], fill='white', outline='lightgray')\n",
        "    \n",
        "    complex_path = \"complex_scene.png\"\n",
        "    img.save(complex_path)\n",
        "    print(f\"âœ… Created complex test image: {complex_path}\")\n",
        "    return complex_path\n",
        "\n",
        "def experiment_with_prompts():\n",
        "    \"\"\"Test different prompting strategies for object detection\"\"\"\n",
        "    \n",
        "    print(\"ðŸ§ª EXPERIMENT: Different Prompting Strategies\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Create our test image\n",
        "    complex_image = create_complex_test_image()\n",
        "    \n",
        "    # Display the image\n",
        "    try:\n",
        "        img = Image.open(complex_image)\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.imshow(img)\n",
        "        plt.title(\"Complex Test Scene\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    except:\n",
        "        print(\"Could not display image\")\n",
        "    \n",
        "    # Different prompting approaches\n",
        "    prompts = {\n",
        "        \"Basic\": \"\"\"Detect and list all objects in this image. For each object, provide:\n",
        "        - Name of the object\n",
        "        - Count of how many you see\n",
        "        - Primary colors\n",
        "        Return as JSON.\"\"\",\n",
        "        \n",
        "        \"Detailed\": \"\"\"You are an expert computer vision system. Analyze this image thoroughly and detect ALL visible objects. \n",
        "        For each object type, provide:\n",
        "        - Specific name (e.g., 'house', 'car', 'tree' not just 'building', 'vehicle', 'plant')\n",
        "        - Exact count of instances\n",
        "        - All visible colors\n",
        "        - Brief description if helpful\n",
        "        Be comprehensive and accurate. Return structured JSON data.\"\"\",\n",
        "        \n",
        "        \"Category-focused\": \"\"\"Analyze this image and categorize all objects you see into these groups:\n",
        "        1. Buildings/Structures\n",
        "        2. Vehicles\n",
        "        3. Nature/Plants\n",
        "        4. Weather/Sky elements\n",
        "        5. Other objects\n",
        "        \n",
        "        For each object, specify its category, name, count, and colors. Return as JSON.\"\"\"\n",
        "    }\n",
        "    \n",
        "    print(\"\\\\nðŸŽ¯ Testing different prompt strategies...\")\n",
        "    \n",
        "    for strategy, prompt in prompts.items():\n",
        "        print(f\"\\\\n--- {strategy} Prompting Strategy ---\")\n",
        "        \n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=\"llama3.2-vision\",\n",
        "                messages=[{\n",
        "                    'role': 'user',\n",
        "                    'content': prompt,\n",
        "                    'images': [complex_image]\n",
        "                }],\n",
        "                format=\"json\",\n",
        "                options={'temperature': 0}\n",
        "            )\n",
        "            \n",
        "            result = response['message']['content']\n",
        "            print(f\"âœ… {strategy} result:\")\n",
        "            \n",
        "            # Try to parse and display nicely\n",
        "            try:\n",
        "                parsed = json.loads(result)\n",
        "                print(json.dumps(parsed, indent=2))\n",
        "            except:\n",
        "                print(result[:200] + \"...\" if len(result) > 200 else result)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error with {strategy}: {e}\")\n",
        "    \n",
        "    return complex_image\n",
        "\n",
        "def compare_models():\n",
        "    \"\"\"Compare different vision models if available\"\"\"\n",
        "    \n",
        "    print(\"\\\\nðŸ¤– EXPERIMENT: Model Comparison\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Check available models\n",
        "    try:\n",
        "        models = ollama.list()\n",
        "        vision_models = [m['model'] for m in models['models'] if 'vision' in m['model'].lower()]\n",
        "        \n",
        "        if len(vision_models) > 1:\n",
        "            print(f\"Found {len(vision_models)} vision models: {vision_models}\")\n",
        "            \n",
        "            # Test each model on the same image\n",
        "            test_image = \"complex_scene.png\"\n",
        "            \n",
        "            for model in vision_models[:2]:  # Test first 2 models\n",
        "                print(f\"\\\\n--- Testing {model} ---\")\n",
        "                try:\n",
        "                    result = detect_objects(test_image, model)\n",
        "                    data = json.loads(result)\n",
        "                    \n",
        "                    if \"objects\" in data:\n",
        "                        print(f\"Detected {len(data['objects'])} object types:\")\n",
        "                        for obj in data['objects']:\n",
        "                            print(f\"  - {obj.get('name', 'Unknown')}: {obj.get('count', 0)}\")\n",
        "                    else:\n",
        "                        print(\"No objects detected\")\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    print(f\"Error: {e}\")\n",
        "        else:\n",
        "            print(f\"Only {len(vision_models)} vision model(s) available: {vision_models}\")\n",
        "            print(\"Install more models to compare: ollama pull llava:latest\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error checking models: {e}\")\n",
        "\n",
        "def test_different_image_types():\n",
        "    \"\"\"Test our system with different types of content\"\"\"\n",
        "    \n",
        "    print(\"\\\\nðŸ“¸ EXPERIMENT: Different Image Types\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Create different test scenarios\n",
        "    test_scenarios = []\n",
        "    \n",
        "    # Scenario 1: Simple geometric shapes\n",
        "    print(\"Creating geometric shapes test...\")\n",
        "    try:\n",
        "        from PIL import Image, ImageDraw\n",
        "        img = Image.new('RGB', (300, 300), 'white')\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        \n",
        "        # Various shapes\n",
        "        draw.rectangle([50, 50, 100, 100], fill='red')\n",
        "        draw.ellipse([150, 50, 200, 100], fill='blue')\n",
        "        draw.polygon([(250, 50), (275, 100), (225, 100)], fill='green')\n",
        "        draw.rectangle([50, 150, 100, 200], fill='yellow')\n",
        "        draw.ellipse([150, 150, 200, 200], fill='purple')\n",
        "        \n",
        "        shapes_path = \"geometric_shapes.png\"\n",
        "        img.save(shapes_path)\n",
        "        test_scenarios.append((\"Geometric Shapes\", shapes_path))\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error creating shapes: {e}\")\n",
        "    \n",
        "    # Test each scenario\n",
        "    for scenario_name, image_path in test_scenarios:\n",
        "        print(f\"\\\\nðŸ” Testing: {scenario_name}\")\n",
        "        try:\n",
        "            result = detect_objects(image_path)\n",
        "            data = json.loads(result)\n",
        "            \n",
        "            if \"objects\" in data and data[\"objects\"]:\n",
        "                print(f\"âœ… Detected {len(data['objects'])} object types:\")\n",
        "                for obj in data[\"objects\"]:\n",
        "                    print(f\"  - {obj.get('name', 'Unknown')}: {obj.get('count', 0)} ({', '.join(obj.get('color', []))})\")\n",
        "            else:\n",
        "                print(\"âŒ No objects detected\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error: {e}\")\n",
        "\n",
        "# Run our experiments\n",
        "print(\"ðŸš€ Starting Object Detection Experiments!\")\n",
        "print(\"\\\\nNote: These experiments will test different aspects of our system:\")\n",
        "print(\"1. Complex scene analysis\")\n",
        "print(\"2. Different prompting strategies\") \n",
        "print(\"3. Model comparisons (if multiple models available)\")\n",
        "print(\"4. Various image types\")\n",
        "\n",
        "# Uncomment the experiments you want to run:\n",
        "\n",
        "# Experiment 1: Complex scene with different prompts\n",
        "# complex_img = experiment_with_prompts()\n",
        "\n",
        "# Experiment 2: Compare different models\n",
        "# compare_models()\n",
        "\n",
        "# Experiment 3: Test different image types\n",
        "# test_different_image_types()\n",
        "\n",
        "print(\"\\\\nðŸ’¡ Uncomment the experiment functions above to run them!\")\n",
        "print(\"Each experiment will teach you something different about computer vision!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Part 6: Advanced Applications & Extensions ðŸš€\n",
        "\n",
        "Now that you understand the fundamentals, let's explore advanced applications and ideas for extending your object detection system!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸš€ ADVANCED OBJECT DETECTION APPLICATIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\"\"\n",
        "ðŸŽ¯ REAL-WORLD APPLICATIONS:\n",
        "\n",
        "1. ðŸª RETAIL & INVENTORY\n",
        "   - Automatic product counting\n",
        "   - Shelf monitoring systems\n",
        "   - Quality control inspection\n",
        "   - Customer behavior analysis\n",
        "\n",
        "2. ðŸš— AUTONOMOUS VEHICLES\n",
        "   - Traffic sign detection\n",
        "   - Pedestrian identification\n",
        "   - Vehicle tracking\n",
        "   - Road condition assessment\n",
        "\n",
        "3. ðŸ¥ HEALTHCARE\n",
        "   - Medical image analysis\n",
        "   - Equipment monitoring\n",
        "   - Patient safety systems\n",
        "   - Diagnostic assistance\n",
        "\n",
        "4. ðŸ”’ SECURITY & SURVEILLANCE\n",
        "   - Intrusion detection\n",
        "   - Crowd monitoring\n",
        "   - Suspicious activity alerts\n",
        "   - Access control systems\n",
        "\n",
        "5. ðŸŒ¾ AGRICULTURE\n",
        "   - Crop monitoring\n",
        "   - Pest detection\n",
        "   - Harvest optimization\n",
        "   - Livestock tracking\n",
        "\n",
        "6. ðŸ­ MANUFACTURING\n",
        "   - Defect detection\n",
        "   - Assembly line monitoring\n",
        "   - Safety compliance\n",
        "   - Quality assurance\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\\\nðŸ› ï¸ TECHNICAL EXTENSIONS YOU CAN BUILD:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "extensions = [\n",
        "    \"ðŸ“± Mobile App Integration\",\n",
        "    \"ðŸŽ¥ Real-time Video Processing\", \n",
        "    \"ðŸ—ƒï¸ Database Storage & Analytics\",\n",
        "    \"ðŸ”” Alert & Notification Systems\",\n",
        "    \"ðŸ“Š Advanced Data Visualization\",\n",
        "    \"ðŸ¤– Multi-Model Ensemble Systems\",\n",
        "    \"ðŸŒ API Development & Integration\",\n",
        "    \"ðŸ“ˆ Performance Monitoring\",\n",
        "    \"ðŸŽ›ï¸ Custom Training Pipelines\",\n",
        "    \"ðŸ”„ Automated Retraining Systems\"\n",
        "]\n",
        "\n",
        "for i, ext in enumerate(extensions, 1):\n",
        "    print(f\"{i:2d}. {ext}\")\n",
        "\n",
        "def create_advanced_detection_system():\n",
        "    \"\"\"\n",
        "    Template for an advanced object detection system with additional features.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"\\\\nðŸ—ï¸ ADVANCED SYSTEM ARCHITECTURE\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    advanced_system_code = '''\n",
        "    class AdvancedObjectDetectionSystem:\n",
        "        \"\"\"\n",
        "        An advanced object detection system with additional capabilities.\n",
        "        \"\"\"\n",
        "        \n",
        "        def __init__(self, models: List[str], confidence_threshold: float = 0.7):\n",
        "            self.models = models\n",
        "            self.confidence_threshold = confidence_threshold\n",
        "            self.detection_history = []\n",
        "            self.performance_metrics = {}\n",
        "            \n",
        "        def multi_model_detection(self, image_path: str) -> Dict[str, Any]:\n",
        "            \"\"\"Use multiple models and combine results for better accuracy.\"\"\"\n",
        "            results = {}\n",
        "            \n",
        "            for model in self.models:\n",
        "                try:\n",
        "                    result = detect_objects(image_path, model)\n",
        "                    results[model] = json.loads(result)\n",
        "                except Exception as e:\n",
        "                    results[model] = {\"error\": str(e)}\n",
        "            \n",
        "            # Combine results using ensemble methods\n",
        "            combined_result = self.ensemble_results(results)\n",
        "            return combined_result\n",
        "            \n",
        "        def ensemble_results(self, results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "            \"\"\"Combine results from multiple models.\"\"\"\n",
        "            # Implement voting, averaging, or other ensemble methods\n",
        "            # This is a simplified version\n",
        "            \n",
        "            all_objects = {}\n",
        "            \n",
        "            for model, result in results.items():\n",
        "                if \"objects\" in result:\n",
        "                    for obj in result[\"objects\"]:\n",
        "                        name = obj.get(\"name\", \"unknown\")\n",
        "                        count = obj.get(\"count\", 0)\n",
        "                        \n",
        "                        if name in all_objects:\n",
        "                            all_objects[name][\"votes\"] += 1\n",
        "                            all_objects[name][\"total_count\"] += count\n",
        "                        else:\n",
        "                            all_objects[name] = {\n",
        "                                \"votes\": 1,\n",
        "                                \"total_count\": count,\n",
        "                                \"colors\": obj.get(\"color\", [])\n",
        "                            }\n",
        "            \n",
        "            # Filter by confidence (number of models that detected the object)\n",
        "            final_objects = []\n",
        "            for name, data in all_objects.items():\n",
        "                confidence = data[\"votes\"] / len(self.models)\n",
        "                if confidence >= self.confidence_threshold:\n",
        "                    final_objects.append({\n",
        "                        \"name\": name,\n",
        "                        \"count\": data[\"total_count\"] // data[\"votes\"],  # Average count\n",
        "                        \"color\": data[\"colors\"],\n",
        "                        \"confidence\": confidence\n",
        "                    })\n",
        "            \n",
        "            return {\"objects\": final_objects}\n",
        "            \n",
        "        def track_performance(self, detection_time: float, accuracy: float):\n",
        "            \"\"\"Track system performance metrics.\"\"\"\n",
        "            self.performance_metrics[\"avg_detection_time\"] = detection_time\n",
        "            self.performance_metrics[\"accuracy\"] = accuracy\n",
        "            \n",
        "        def save_detection_history(self, image_path: str, results: Dict[str, Any]):\n",
        "            \"\"\"Save detection results for analysis.\"\"\"\n",
        "            self.detection_history.append({\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"image_path\": image_path,\n",
        "                \"results\": results\n",
        "            })\n",
        "    '''\n",
        "    \n",
        "    print(\"This advanced system includes:\")\n",
        "    print(\"âœ… Multi-model ensemble detection\")\n",
        "    print(\"âœ… Confidence-based filtering\")\n",
        "    print(\"âœ… Performance tracking\")\n",
        "    print(\"âœ… Detection history logging\")\n",
        "    print(\"âœ… Result combination algorithms\")\n",
        "\n",
        "def create_specialized_detectors():\n",
        "    \"\"\"Examples of specialized detection systems.\"\"\"\n",
        "    \n",
        "    print(\"\\\\nðŸŽ¯ SPECIALIZED DETECTION SYSTEMS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    specialized_examples = {\n",
        "        \"Safety Detector\": {\n",
        "            \"description\": \"Detects safety equipment and hazards\",\n",
        "            \"prompt\": \"\"\"Analyze this image for safety-related items:\n",
        "            - Personal protective equipment (helmets, gloves, safety vests)\n",
        "            - Safety hazards (spills, obstacles, dangerous equipment)\n",
        "            - Emergency equipment (fire extinguishers, first aid kits)\n",
        "            - Safety compliance indicators\n",
        "            \n",
        "            For each item, specify if it represents good safety practices or potential hazards.\"\"\"\n",
        "        },\n",
        "        \n",
        "        \"Food Analyzer\": {\n",
        "            \"description\": \"Analyzes food items and nutritional content\",\n",
        "            \"prompt\": \"\"\"Identify all food items in this image:\n",
        "            - Specific food names (not just 'food')\n",
        "            - Estimated quantities/portions\n",
        "            - Food categories (fruits, vegetables, proteins, etc.)\n",
        "            - Freshness indicators\n",
        "            - Nutritional classifications (healthy, processed, etc.)\"\"\"\n",
        "        },\n",
        "        \n",
        "        \"Document Scanner\": {\n",
        "            \"description\": \"Identifies and categorizes documents\",\n",
        "            \"prompt\": \"\"\"Analyze this image for documents and text:\n",
        "            - Document types (forms, letters, receipts, etc.)\n",
        "            - Text elements (headers, paragraphs, tables)\n",
        "            - Document condition (quality, completeness)\n",
        "            - Important visual elements (logos, signatures, stamps)\"\"\"\n",
        "        },\n",
        "        \n",
        "        \"Room Analyzer\": {\n",
        "            \"description\": \"Analyzes room layout and furniture\",\n",
        "            \"prompt\": \"\"\"Analyze this room image:\n",
        "            - Furniture items and their conditions\n",
        "            - Room type identification\n",
        "            - Layout and organization\n",
        "            - Lighting conditions\n",
        "            - Cleanliness and maintenance indicators\n",
        "            - Accessibility features\"\"\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    for name, details in specialized_examples.items():\n",
        "        print(f\"\\\\nðŸ” {name}:\")\n",
        "        print(f\"   Purpose: {details['description']}\")\n",
        "        print(f\"   Specialized prompt focuses on domain-specific detection\")\n",
        "\n",
        "def integration_examples():\n",
        "    \"\"\"Show how to integrate with other systems.\"\"\"\n",
        "    \n",
        "    print(\"\\\\nðŸ”— INTEGRATION EXAMPLES\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    integrations = [\n",
        "        {\n",
        "            \"name\": \"Database Integration\",\n",
        "            \"description\": \"Store detection results in databases\",\n",
        "            \"code\": \"\"\"\n",
        "            import sqlite3\n",
        "            \n",
        "            def save_to_database(image_path, detection_results):\n",
        "                conn = sqlite3.connect('detections.db')\n",
        "                cursor = conn.cursor()\n",
        "                \n",
        "                cursor.execute('''\n",
        "                    INSERT INTO detections (image_path, timestamp, results)\n",
        "                    VALUES (?, ?, ?)\n",
        "                ''', (image_path, datetime.now(), json.dumps(detection_results)))\n",
        "                \n",
        "                conn.commit()\n",
        "                conn.close()\n",
        "            \"\"\"\n",
        "        },\n",
        "        \n",
        "        {\n",
        "            \"name\": \"API Development\",\n",
        "            \"description\": \"Create REST API for detection service\",\n",
        "            \"code\": \"\"\"\n",
        "            from flask import Flask, request, jsonify\n",
        "            \n",
        "            app = Flask(__name__)\n",
        "            \n",
        "            @app.route('/detect', methods=['POST'])\n",
        "            def detect_api():\n",
        "                if 'image' not in request.files:\n",
        "                    return jsonify({'error': 'No image provided'}), 400\n",
        "                \n",
        "                image = request.files['image']\n",
        "                # Save temporarily and process\n",
        "                result = detect_objects(image_path)\n",
        "                return jsonify(json.loads(result))\n",
        "            \"\"\"\n",
        "        },\n",
        "        \n",
        "        {\n",
        "            \"name\": \"Alert System\",\n",
        "            \"description\": \"Send notifications based on detections\",\n",
        "            \"code\": \"\"\"\n",
        "            import smtplib\n",
        "            from email.mime.text import MIMEText\n",
        "            \n",
        "            def send_alert(detection_results, alert_conditions):\n",
        "                for obj in detection_results.get('objects', []):\n",
        "                    if obj['name'] in alert_conditions:\n",
        "                        send_email_alert(f\"Detected: {obj['name']}\")\n",
        "            \"\"\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    for integration in integrations:\n",
        "        print(f\"\\\\nðŸ“¡ {integration['name']}:\")\n",
        "        print(f\"   {integration['description']}\")\n",
        "\n",
        "# Show all the advanced concepts\n",
        "create_advanced_detection_system()\n",
        "create_specialized_detectors()\n",
        "integration_examples()\n",
        "\n",
        "print(\"\\\\nðŸŽ“ LEARNING OBJECTIVES ACHIEVED:\")\n",
        "print(\"âœ… Understanding Vision Language Models\")\n",
        "print(\"âœ… Structured data extraction from images\")\n",
        "print(\"âœ… Data visualization and analysis\")\n",
        "print(\"âœ… Web application development\")\n",
        "print(\"âœ… System architecture and design\")\n",
        "print(\"âœ… Advanced applications and extensions\")\n",
        "\n",
        "print(\"\\\\nðŸš€ You're now ready to build sophisticated computer vision applications!\")\n",
        "print(\"Choose an extension idea and start building your own advanced system!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ðŸŽ¯ Summary & Next Steps\n",
        "\n",
        "Congratulations! You've just built a complete object detection system using Vision Language Models! ðŸŽ‰\n",
        "\n",
        "### What You've Learned:\n",
        "\n",
        "1. **ðŸ‘ï¸ Vision Language Models**: How AI can \"see\" and understand images\n",
        "2. **ðŸ“Š Structured Outputs**: Extracting organized data from AI vision systems\n",
        "3. **ðŸŽ¨ Data Visualization**: Creating charts and graphs from detection results\n",
        "4. **ðŸŒ Web Applications**: Building user-friendly interfaces with Streamlit\n",
        "5. **ðŸ—ï¸ System Architecture**: Organizing code into modular, reusable components\n",
        "6. **ðŸ”¬ Experimentation**: Testing different approaches and analyzing results\n",
        "\n",
        "### Key Components You Built:\n",
        "\n",
        "- **Vision Model Interface**: Communication with Ollama vision models\n",
        "- **Data Structures**: Pydantic models for consistent, structured outputs\n",
        "- **Visualization System**: Charts and graphs for detection results\n",
        "- **Web Application**: Complete Streamlit interface for image upload and analysis\n",
        "- **Experiment Framework**: Tools for testing and comparing different approaches\n",
        "- **Extension Templates**: Advanced features and integration examples\n",
        "\n",
        "### Real-World Applications:\n",
        "\n",
        "Your object detection system can be applied to:\n",
        "- **Retail**: Inventory management and customer analytics\n",
        "- **Security**: Surveillance and monitoring systems\n",
        "- **Healthcare**: Medical image analysis and equipment tracking\n",
        "- **Manufacturing**: Quality control and safety monitoring\n",
        "- **Agriculture**: Crop monitoring and livestock management\n",
        "- **Education**: Interactive learning and research tools\n",
        "\n",
        "### Your Next Challenge:\n",
        "\n",
        "Pick one of these extension projects to continue your learning:\n",
        "\n",
        "**Beginner-Friendly:**\n",
        "- Create specialized detectors for specific domains (food, safety, documents)\n",
        "- Add more visualization types (heatmaps, trends, comparisons)\n",
        "- Build a mobile-friendly web interface\n",
        "\n",
        "**Intermediate:**\n",
        "- Implement multi-model ensemble systems\n",
        "- Add database storage and historical analysis\n",
        "- Create REST APIs for integration with other systems\n",
        "\n",
        "**Advanced:**\n",
        "- Build real-time video processing capabilities\n",
        "- Implement custom training pipelines\n",
        "- Create automated monitoring and alert systems\n",
        "\n",
        "### Resources for Continued Learning:\n",
        "\n",
        "- **Computer Vision**: OpenCV, YOLO, Detectron2\n",
        "- **Deep Learning**: PyTorch, TensorFlow, Hugging Face\n",
        "- **Web Development**: FastAPI, React, Vue.js\n",
        "- **Deployment**: Docker, AWS, Google Cloud, Azure\n",
        "\n",
        "Keep experimenting, keep building, and most importantly - keep learning! The world of AI vision is vast and full of exciting possibilities! ðŸš€âœ¨\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Object Detection with Vision Language Models ðŸ‘ï¸ðŸ¤–\n",
        "\n",
        "Welcome to an exciting journey into computer vision and AI! In this tutorial, we'll learn how to build an object detection system using vision-language models that can \"see\" and understand images.\n",
        "\n",
        "## What You'll Learn ðŸŽ¯\n",
        "\n",
        "In this comprehensive tutorial, you'll discover:\n",
        "\n",
        "1. **ðŸ‘ï¸ Computer Vision Basics**: How AI can analyze and understand images\n",
        "2. **ðŸ§  Vision-Language Models**: AI models that combine visual and text understanding\n",
        "3. **ðŸ” Object Detection**: Identifying and counting objects in images\n",
        "4. **ðŸ“Š Structured Data Extraction**: Getting organized information from AI responses\n",
        "5. **ðŸŽ¨ Data Visualization**: Displaying results in user-friendly formats\n",
        "6. **ðŸ’» Interactive Applications**: Building a complete image analysis tool\n",
        "7. **ðŸŒˆ Color Analysis**: Understanding how AI perceives colors in objects\n",
        "\n",
        "Let's dive into the fascinating world of AI-powered image analysis! ðŸš€\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Prerequisites ðŸ› ï¸\n",
        "\n",
        "Before we start building our object detection system, let's make sure we have everything we need:\n",
        "\n",
        "### What We Need:\n",
        "- **Ollama** with a vision model (like llama3.2-vision)\n",
        "- **Python packages**: ollama, pydantic, streamlit, PIL, pandas, json, matplotlib\n",
        "- **Sample images** to test our system\n",
        "\n",
        "Let's install the required packages and set up our imports:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# %pip install ollama pydantic streamlit pillow pandas matplotlib\n",
        "\n",
        "# Import all necessary libraries\n",
        "import ollama\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"âœ… All packages imported!\")\n",
        "print(\"ðŸŽ¯ Ready to build our object detection system!\")\n",
        "\n",
        "# Check if we have access to Ollama\n",
        "try:\n",
        "    models = ollama.list()\n",
        "    print(f\"\\\\nðŸ¤– Available Ollama models: {len(models['models'])}\")\n",
        "    \n",
        "    # Look for vision models\n",
        "    vision_models = [m for m in models['models'] if 'vision' in m['model'].lower()]\n",
        "    if vision_models:\n",
        "        print(f\"ðŸ‘ï¸ Vision models found: {[m['model'] for m in vision_models]}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No vision models found. You may need to install one:\")\n",
        "        print(\"   Run: ollama pull llama3.2-vision\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error connecting to Ollama: {e}\")\n",
        "    print(\"Make sure Ollama is running!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install ollama pydantic streamlit pillow pandas matplotlib\n",
        "\n",
        "# Import all necessary libraries\n",
        "import ollama\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"âœ… All packages installed and imported!\")\n",
        "print(\"ðŸŽ¯ Ready to build our object detection system!\")\n",
        "\n",
        "# Check if we have access to Ollama\n",
        "try:\n",
        "    models = ollama.list()\n",
        "    print(f\"\\\\nðŸ¤– Available Ollama models: {len(models['models'])}\")\n",
        "    \n",
        "    # Look for vision models\n",
        "    vision_models = [m for m in models['models'] if 'vision' in m['model'].lower()]\n",
        "    if vision_models:\n",
        "        print(f\"ðŸ‘ï¸ Vision models found: {[m['model'] for m in vision_models]}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No vision models found. You may need to install one:\")\n",
        "        print(\"   Run: ollama pull llama3.2-vision\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error connecting to Ollama: {e}\")\n",
        "    print(\"Make sure Ollama is running!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
